<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>cs.AI Daily â€¢ arXiv CS Daily</title>
  <link rel="stylesheet" href="../assets/styles.css" />
  <script defer src="../assets/site.js"></script>
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">
        <a href="../index.html" class="title">arXiv CS Daily</a>
        <div class="subtitle">Static demo built from arXiv Atom API (prefetched to avoid CORS)</div>
      </div>
      <nav class="nav">
        <a href="../index.html">Home</a>
        <a href="../categories/cs.AI.html" class="active">cs.AI</a>
        <a href="../categories/cs.AR.html">cs.AR</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <div class="card">
      <div class="card-body">
        <h2>Artificial Intelligence (cs.AI)</h2>
        <div class="muted">Newest 3 submissions (sorted by submittedDate, descending).</div>
      </div>

      <div class="paper-item">
        <div class="paper-title"><a href="../papers/cs.AI-1.html">SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model</a></div>
        <div class="paper-meta">
          <span class="pill"><strong>Published</strong> 2025-12-11 18:59:56 UTC</span>
          <span class="pill"><strong>Updated</strong> 2025-12-11 18:59:56 UTC</span>
        </div>
        <div class="muted" style="margin-bottom:10px"><strong>Authors:</strong> Yukai Shi, Weiyu Li, Zihao Wang, Hongyang Li, Xingyu Chen, Ping Tan, Lei Zhang</div>
        <p class="paper-abstract">We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.</p>
        <div class="paper-actions">
          <a class="btn small primary" href="../papers/cs.AI-1.html">Details</a>
          <a class="btn small" href="https://arxiv.org/pdf/2512.10957v1" target="_blank" rel="noopener">PDF</a>
          <a class="btn small" href="https://arxiv.org/abs/2512.10957v1" target="_blank" rel="noopener">arXiv</a>
        </div>
      </div>

      <div class="paper-item">
        <div class="paper-title"><a href="../papers/cs.AI-2.html">Hierarchical Dataset Selection for High-Quality Data Sharing</a></div>
        <div class="paper-meta">
          <span class="pill"><strong>Published</strong> 2025-12-11 18:59:55 UTC</span>
          <span class="pill"><strong>Updated</strong> 2025-12-11 18:59:55 UTC</span>
        </div>
        <div class="muted" style="margin-bottom:10px"><strong>Authors:</strong> Xiaona Zhou, Yingyan Zeng, Ran Jin, Ismini Lourentzou</div>
        <p class="paper-abstract">The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.</p>
        <div class="paper-actions">
          <a class="btn small primary" href="../papers/cs.AI-2.html">Details</a>
          <a class="btn small" href="https://arxiv.org/pdf/2512.10952v1" target="_blank" rel="noopener">PDF</a>
          <a class="btn small" href="https://arxiv.org/abs/2512.10952v1" target="_blank" rel="noopener">arXiv</a>
        </div>
      </div>

      <div class="paper-item">
        <div class="paper-title"><a href="../papers/cs.AI-3.html">Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation</a></div>
        <div class="paper-meta">
          <span class="pill"><strong>Published</strong> 2025-12-11 18:59:52 UTC</span>
          <span class="pill"><strong>Updated</strong> 2025-12-11 18:59:52 UTC</span>
        </div>
        <div class="muted" style="margin-bottom:10px"><strong>Authors:</strong> Yiwen Tang, Zoey Guo, Kaixin Zhu, Ray Zhang, Qizhi Chen, Dongzhi Jiang, Junli Liu, Bohan Zeng, Haoming Song, Delin Qu, Tianyi Bai, Dan Xu, Wentao Zhang, Bin Zhao</div>
        <p class="paper-abstract">Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.</p>
        <div class="paper-actions">
          <a class="btn small primary" href="../papers/cs.AI-3.html">Details</a>
          <a class="btn small" href="https://arxiv.org/pdf/2512.10949v1" target="_blank" rel="noopener">PDF</a>
          <a class="btn small" href="https://arxiv.org/abs/2512.10949v1" target="_blank" rel="noopener">arXiv</a>
        </div>
      </div>
    </div>

    <div class="footer">
      <div class="note">Data source: <a href="https://export.arxiv.org/api/query">arXiv API</a>. Pages are generated by <code>scripts/fetch_arxiv.mjs</code>.</div>
    </div>
  </main>
</body>
</html>
